{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turtle runs - RL with the smoothed Crossentropy Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['N', 'E', 'S', 'W', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Turtle():\n",
    "    def __init__(self, position, halite):\n",
    "        self.position = position\n",
    "        self.halite = halite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameState():\n",
    "    def __init__(self, game_map, position, halite):\n",
    "        self.game_map = game_map\n",
    "        self.position = position\n",
    "        self.halite = halite\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return (self.game_map == other.game_map).all() and self.position == other.position and self.halite == other.halite\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHalite():\n",
    "    def __init__(self, height, width, start_pos):\n",
    "        np.random.seed(42)\n",
    "        self.game_map = np.random.randint(1, 1000, size=(height, width))\n",
    "        self.game_map[start_pos] = 0\n",
    "        self.orig_map = self.game_map.copy()\n",
    "        self.turtle = Turtle(start_pos, 0)\n",
    "        self.turn = 1\n",
    "        self.max_turns = 50\n",
    "        self.halite = 0\n",
    "        self.base = start_pos\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "    \n",
    "    def get_state(self):\n",
    "        game_state = GameState(self.game_map.copy(), self.turtle.position, self.turtle.halite)\n",
    "        return game_state, self.turn, self.halite, self.turn == self.max_turns\n",
    "    \n",
    "    def reset(self):\n",
    "        self.game_map = self.orig_map.copy()\n",
    "        self.turtle = Turtle(self.base, 0)\n",
    "        self.turn = 1\n",
    "        self.halite = 0\n",
    "        \n",
    "    def step(self, action):\n",
    "        reward = 0\n",
    "        if action == 'O':\n",
    "            mined_halite = self.game_map[self.turtle.position] // 4\n",
    "            self.game_map[self.turtle.position] -= mined_halite\n",
    "            self.turtle.halite += min(1000, mined_halite)\n",
    "        else:\n",
    "            if action == 'N':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (0, 1))])\n",
    "            elif action == 'E':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (1, 0))])\n",
    "            elif action == 'S':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (0, -1))])\n",
    "            elif action == 'W':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (-1, 0))])\n",
    "            #print(cost_halite, self.turtle.halite)\n",
    "            if cost_halite <= self.turtle.halite:\n",
    "                #print(\"moving turtle to {}\".format(new_pos))\n",
    "                self.turtle = Turtle(new_pos, self.turtle.halite - cost_halite)\n",
    "            else:\n",
    "                mined_halite = self.game_map[self.turtle.position] // 4\n",
    "                self.game_map[self.turtle.position] -= mined_halite\n",
    "                self.turtle.halite += min(1000, mined_halite)                \n",
    "        self.turtle.position = (self.turtle.position[0] % self.width, \n",
    "                                self.turtle.position[1] % self.height)\n",
    "        if self.turtle.position == self.base:\n",
    "            self.halite += self.turtle.halite\n",
    "            reward = self.turtle.halite\n",
    "            self.turtle.halite = 0\n",
    "        self.turn += 1\n",
    "        return self.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(game_state, turn, policy_mapping=None):\n",
    "    if policy_mapping is not None and (game_state.position, game_state.halite) in policy_mapping:\n",
    "        for map_action in policy_mapping[(game_state.position, game_state.halite)]:\n",
    "            if (map_action[0] == game_state.game_map).all():\n",
    "                return np.random.choice(actions, p=map_action[1])\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be a tuple rather than a list as start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = SimpleHalite(5, 5, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 200\n",
    "M = 50\n",
    "alpha = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_policy_mapping(policy_mapping, state, action_probs):\n",
    "    if (state.position, state.halite) not in policy_mapping:\n",
    "            policy_mapping[(state.position, state.halite)] = []\n",
    "    policy_mapping[(state.position, state.halite)].append((state.game_map, action_probs))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 220.985\n",
      "Entries in policy mapping: 0\n",
      "Number of keys: 0\n",
      "Updated: 0, not updated: 6878\n",
      "Iteration took 267 seconds\n",
      "1 231.015\n",
      "Entries in policy mapping: 6878\n",
      "Number of keys: 4315\n",
      "Updated: 321, not updated: 6849\n",
      "Iteration took 265 seconds\n",
      "2 243.99\n",
      "Entries in policy mapping: 13727\n",
      "Number of keys: 6740\n",
      "Updated: 377, not updated: 6790\n",
      "Iteration took 266 seconds\n",
      "3 238.88\n",
      "Entries in policy mapping: 20517\n",
      "Number of keys: 8244\n",
      "Updated: 493, not updated: 6741\n",
      "Iteration took 265 seconds\n",
      "4 235.185\n",
      "Entries in policy mapping: 27258\n",
      "Number of keys: 9198\n",
      "Updated: 553, not updated: 6643\n",
      "Iteration took 260 seconds\n",
      "5 259.62\n",
      "Entries in policy mapping: 33901\n",
      "Number of keys: 9887\n",
      "Updated: 578, not updated: 6629\n",
      "Iteration took 260 seconds\n",
      "6 297.55\n",
      "Entries in policy mapping: 40530\n",
      "Number of keys: 10611\n",
      "Updated: 654, not updated: 6454\n",
      "Iteration took 253 seconds\n",
      "7 301.155\n",
      "Entries in policy mapping: 46984\n",
      "Number of keys: 11118\n"
     ]
    }
   ],
   "source": [
    "policy_mapping = {}\n",
    "for j in range(M):\n",
    "    now = time()\n",
    "    state_action_maps = []\n",
    "    final_rewards = []\n",
    "    for i in range(N):\n",
    "        state_action_map = []\n",
    "        game.reset()\n",
    "        turn_count = 0\n",
    "        game_state, turn, reward, done = game.get_state()\n",
    "        while not done:\n",
    "            action = policy(game_state, turn, policy_mapping)\n",
    "            state_action_map.append((game_state, action))\n",
    "            turn_count += 1\n",
    "            game_state, turn, reward, done = game.step(action)\n",
    "        state_action_maps.append(state_action_map)\n",
    "        final_rewards.append(reward)\n",
    "    print(j, np.mean(final_rewards))\n",
    "    elite_games = np.argsort(final_rewards)[N//5:]\n",
    "    maps_to_keep = np.array(state_action_maps)[elite_games]\n",
    "    full_map = [item for sublist in maps_to_keep for item in sublist]\n",
    "    done_policies = []\n",
    "    print(\"Entries in policy mapping:\", sum(len(x) for x in policy_mapping.values()))\n",
    "    print(\"Number of keys:\", len(policy_mapping.keys()))\n",
    "    updated, not_updated = 0, 0\n",
    "    for (state, _) in full_map:\n",
    "        if state not in done_policies:\n",
    "            done_policies.append(state)\n",
    "            subset = [a for (s, a) in full_map if s == state]\n",
    "            action_probs = []\n",
    "            action_probs_raw = []\n",
    "            for action in actions:\n",
    "                action_probs.append((subset.count(action) + 1) / (len(subset) + 5))\n",
    "                action_probs_raw.append(subset.count(action) / len(subset))\n",
    "            if (state.position, state.halite) in policy_mapping: \n",
    "                for ix, (game_map, probs) in enumerate(policy_mapping[state.position, state.halite]):\n",
    "                    if (game_map == state.game_map).all():\n",
    "                        policy_mapping[(state.position, state.halite)][ix] = \\\n",
    "                            (state.game_map, alpha * np.array(action_probs_raw) + (1 - alpha) * np.array(probs))\n",
    "                        updated += 1\n",
    "                        break\n",
    "                not_updated += 1\n",
    "                add_to_policy_mapping(policy_mapping, state, action_probs)\n",
    "            else:\n",
    "                not_updated += 1\n",
    "                add_to_policy_mapping(policy_mapping, state, action_probs)\n",
    "    print(\"Updated: {}, not updated: {}\".format(updated, not_updated))\n",
    "    print(\"Iteration took {:.0f} seconds\".format(time() - now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[x for x in policy_mapping if max(x[1]) > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now simulate the game loop. We now need to \n",
    "- fix the game map (otherwise the state mappong will become too unwieldy) - DONE\n",
    "- keep track of state-action pairs (memory intensive, but maybe we can manage) - DONE\n",
    "- use state-action pairs from elite runs to update policy - DONE\n",
    "- add smoothing - DONE\n",
    "- improve policy mapping data structure to keep turn times roughly constant - DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
