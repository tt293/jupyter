{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turtle runs - RL with deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from time import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['N', 'E', 'S', 'W', 'O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Turtle():\n",
    "    def __init__(self, position, halite):\n",
    "        self.position = position\n",
    "        self.halite = halite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameState():\n",
    "    def __init__(self, game_map, position, halite, turn):\n",
    "        self.game_map = game_map\n",
    "        self.position = position\n",
    "        self.halite = halite\n",
    "        self.turn = turn\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return (self.game_map == other.game_map).all() and self.position == other.position and self.halite == other.halite and self.turn == other.turn\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.numbers\n",
    "    \n",
    "    def __bytes__(self):\n",
    "        return bytes(self.game_map) + bytes(self.position) + bytes(self.halite)\n",
    "\n",
    "    def __hash__(self):\n",
    "        xxh64 = xxhash.xxh64(self.__bytes__())\n",
    "        return xxh64.intdigest()\n",
    "    \n",
    "    def get_nn_repr(self):\n",
    "        hal_std_scaled = (self.game_map.reshape(-1,) - 500) / 1000\n",
    "        pos_indicator = [0] * 25\n",
    "        pos_indicator[self.position[0] * 5 + self.position[1]] = 1\n",
    "        return np.array(list(hal_std_scaled) + pos_indicator + [self.halite / 1000, self.turn / 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleHalite():\n",
    "    def __init__(self, height, width, start_pos):\n",
    "        np.random.seed(42)\n",
    "        self.game_map = np.random.randint(1, 1000, size=(height, width))\n",
    "        self.game_map[start_pos] = 0\n",
    "        self.orig_map = self.game_map.copy()\n",
    "        self.turtle = Turtle(start_pos, 0)\n",
    "        self.turn = 1\n",
    "        self.max_turns = 50\n",
    "        self.halite = 0\n",
    "        self.base = start_pos\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.actions = ['N', 'E', 'S', 'W', 'O']\n",
    "    \n",
    "    def get_state(self):\n",
    "        game_state = GameState(self.game_map.copy(), self.turtle.position, self.turtle.halite, self.turn)\n",
    "        return game_state\n",
    "    \n",
    "    def get_possible_actions(self):\n",
    "        return self.actions\n",
    "    \n",
    "    def reset(self):\n",
    "        self.game_map = self.orig_map.copy()\n",
    "        self.turtle = Turtle(self.base, 0)\n",
    "        self.turn = 1\n",
    "        self.halite = 0\n",
    "        return self.get_state()\n",
    "        \n",
    "    def step(self, action):\n",
    "        action = actions[action]\n",
    "        reward = 0\n",
    "        if action == 'O':\n",
    "            mined_halite = self.game_map[self.turtle.position] // 4\n",
    "            self.game_map[self.turtle.position] -= mined_halite\n",
    "            self.turtle.halite += min(1000, mined_halite)\n",
    "        else:\n",
    "            if action == 'N':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (-1, 0))])\n",
    "            elif action == 'E':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (0, 1))])\n",
    "            elif action == 'S':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (1, 0))])\n",
    "            elif action == 'W':\n",
    "                cost_halite = self.game_map[self.turtle.position] // 10\n",
    "                new_pos = tuple([sum(x) for x in zip(self.turtle.position, (0, -1))])\n",
    "            #print(cost_halite, self.turtle.halite)\n",
    "            if cost_halite <= self.turtle.halite:\n",
    "                #print(\"moving turtle to {}\".format(new_pos))\n",
    "                self.turtle = Turtle(new_pos, self.turtle.halite - cost_halite)\n",
    "            else:\n",
    "                mined_halite = min(1000 - self.turtle.halite, self.game_map[self.turtle.position] // 4)\n",
    "                self.game_map[self.turtle.position] -= mined_halite\n",
    "                self.turtle.halite += mined_halite\n",
    "        self.turtle.position = (self.turtle.position[0] % self.width, \n",
    "                                self.turtle.position[1] % self.height)\n",
    "        if self.turtle.position == self.base:\n",
    "            self.halite += self.turtle.halite\n",
    "            reward = self.turtle.halite\n",
    "            self.turtle.halite = 0\n",
    "        self.turn += 1\n",
    "        #print(\"turn increment to {}\".format(self.turn))\n",
    "        return self.get_state(), reward, self.turn == self.max_turns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Needs to be a tuple rather than a list as start_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = SimpleHalite(5, 5, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.GameState at 0x198e1643780>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = (len(game.reset().get_nn_repr()),)\n",
    "n_actions = len(game.get_possible_actions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(input_shape=state_dim))\n",
    "network.add(L.Dense(256, activation='relu'))\n",
    "network.add(L.Dense(256, activation='relu'))\n",
    "network.add(L.Dense(len(game.get_possible_actions())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n",
    "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n",
    "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 1.0\n",
    "predicted_next_qvalues = network(next_states_ph)\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
    "# last time step\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    \"\"\"\n",
    "    q_values = network.predict(state.reshape(1, -1))\n",
    "    \n",
    "    if np.random.uniform() > epsilon:\n",
    "        chosen_action = np.argmax(q_values)\n",
    "    else:\n",
    "        chosen_action = np.random.choice(n_actions)\n",
    "    \n",
    "    return chosen_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session(game, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = game.reset().get_nn_repr()\n",
    "    \n",
    "    done = False\n",
    "    while not done:\n",
    "        a = get_action(s, epsilon=epsilon)   \n",
    "        next_s, r, done, = game.step(a)\n",
    "        if train is False:\n",
    "            print(actions[a], next_s.position, total_reward)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s.get_nn_repr()], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s.get_nn_repr()\n",
    "    if train is False:\n",
    "        print(next_s.game_map)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_qlearning_agent(game, epsilon, n_iter, n_sessions):\n",
    "    for j in range(n_iter):   \n",
    "        now = time()\n",
    "        session_rewards = [generate_session(game=game, epsilon=epsilon, train=True) for _ in range(n_sessions)]\n",
    "        print(\"Iteration took {:.0f} seconds, epsilon was {:.3f}\".format(time() - now, epsilon))\n",
    "        print(\"Mean reward on iteration {}: {}\".format(j, np.mean(session_rewards)))\n",
    "        if epsilon > 0.01:\n",
    "            epsilon *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_qlearning_agent(game, 0.5, 100, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E (2, 3) 0\n",
      "S (2, 3) 0\n",
      "S (3, 3) 0\n",
      "O (3, 3) 0\n",
      "O (3, 3) 0\n",
      "O (3, 3) 0\n",
      "O (3, 3) 0\n",
      "W (3, 2) 0\n",
      "O (3, 2) 0\n",
      "O (3, 2) 0\n",
      "O (3, 2) 0\n",
      "O (3, 2) 0\n",
      "O (3, 2) 0\n",
      "S (4, 2) 0\n",
      "O (4, 2) 0\n",
      "O (4, 2) 0\n",
      "O (4, 2) 0\n",
      "O (4, 2) 0\n",
      "O (4, 2) 0\n",
      "S (0, 2) 0\n",
      "O (0, 2) 0\n",
      "O (0, 2) 0\n",
      "O (0, 2) 0\n",
      "O (0, 2) 0\n",
      "O (0, 2) 0\n",
      "S (1, 2) 0\n",
      "W (1, 1) 0\n",
      "O (1, 1) 0\n",
      "O (1, 1) 0\n",
      "O (1, 1) 0\n",
      "O (1, 1) 0\n",
      "E (1, 2) 0\n",
      "S (2, 2) 0\n",
      "E (2, 3) 2812\n",
      "O (2, 3) 2812\n",
      "O (2, 3) 2812\n",
      "W (2, 2) 2812\n",
      "E (2, 3) 2943\n",
      "W (2, 3) 2943\n",
      "W (2, 2) 2943\n",
      "E (2, 3) 2977\n",
      "W (2, 3) 2977\n",
      "W (2, 2) 2977\n",
      "E (2, 3) 3002\n",
      "W (2, 3) 3002\n",
      "W (2, 2) 3002\n",
      "N (1, 2) 3021\n",
      "S (1, 2) 3021\n",
      "S (2, 2) 3021\n",
      "[[103 436 205 271 107]\n",
      " [ 72 223  16 615 122]\n",
      " [467 215   0  84  88]\n",
      " [373 100 208 211 131]\n",
      " [662 309 184 344 492]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3025"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_session(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 2) 872\n",
      "0 (3, 2) 654\n",
      "0 (3, 2) 491\n",
      "0 (3, 2) 369\n",
      "0 (3, 2) 277\n",
      "0 (3, 2) 208\n",
      "0 (3, 2) 156\n",
      "0 (3, 2) 117\n",
      "744 (2, 2) 0\n",
      "744 (3, 2) 117\n",
      "744 (3, 2) 88\n",
      "744 (4, 2) 770\n",
      "744 (4, 2) 578\n",
      "744 (4, 2) 434\n",
      "744 (4, 2) 326\n",
      "744 (4, 2) 245\n",
      "744 (4, 2) 184\n",
      "744 (4, 2) 138\n",
      "744 (4, 2) 104\n",
      "744 (3, 2) 88\n",
      "1413 (2, 2) 0\n",
      "1413 (1, 2) 21\n",
      "1413 (1, 2) 16\n",
      "1413 (0, 2) 861\n",
      "1413 (0, 2) 646\n",
      "1413 (0, 2) 485\n",
      "1413 (0, 2) 364\n",
      "1413 (0, 2) 273\n",
      "1413 (0, 2) 205\n",
      "1413 (0, 2) 154\n",
      "1413 (1, 2) 16\n",
      "2108 (2, 2) 0\n"
     ]
    }
   ],
   "source": [
    "game.reset()\n",
    "total_reward = 0\n",
    "action_seq = [2, 4, 4, 4, 4, 4, 4, 4, 0, 2, 0, 2, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 4, 0, 4, 4, 4, 4, 4, 4, 2, 2]\n",
    "for a in action_seq:\n",
    "    s, r, done = game.step(a)\n",
    "    total_reward += r \n",
    "    print(total_reward, s.position, s.game_map[s.position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103, 436, 154, 271, 107],\n",
       "       [ 72, 701,  12, 615, 122],\n",
       "       [467, 215,   0, 459,  88],\n",
       "       [373, 100,  13, 664, 131],\n",
       "       [662, 309, 770, 344, 492]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.game_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103, 436, 861, 271, 107],\n",
       "       [ 72, 701,  21, 615, 122],\n",
       "       [467, 215,   0, 459,  88],\n",
       "       [373, 100, 872, 664, 131],\n",
       "       [662, 309, 770, 344, 492]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.reset().game_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
